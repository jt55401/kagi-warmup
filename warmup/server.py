# server.py

import os
import hashlib
import logging
from flask import Flask, jsonify, request
from sentence_transformers import SentenceTransformer, util
from concurrent.futures import ThreadPoolExecutor
import jinja2
import numpy as np
import requests
from bs4 import BeautifulSoup
import torch


logging.basicConfig(level=logging.INFO)

if not os.path.exists("cache"):
    os.makedirs("cache")

app = Flask(__name__)

templateLoader = jinja2.FileSystemLoader(searchpath="./templates/")
templateEnv = jinja2.Environment(loader=templateLoader)

index_template = templateEnv.get_template("index.j2")
results_template = templateEnv.get_template("results.j2")

model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")


def crawl_page(url):
    """
    Crawls a web page and retrieves its text content, caching the result locally.

    Args:
        url (str): The URL of the web page to crawl.

    Returns:
        str or None: The text content of the web page if successfully retrieved,
        or None if the URL is invalid, the page cannot be accessed, or an error occurs.

    Behavior:
        - If the URL is None, returns None immediately.
        - Computes a hash of the URL to create a unique cache file path.
        - Checks if the cache file exists:
            - If it exists and contains text, returns the cached content.
            - If it exists but is empty, returns None.
        - If the cache file does not exist:
            - Attempts to fetch the page using an HTTP GET request.
            - If the response status code is not 200, writes an empty cache file and returns None.
            - Parses the page content using BeautifulSoup to extract text.
            - Writes the extracted text to the cache file and returns it.
        - Logs the crawling process and handles exceptions by logging errors,
          writing an empty cache file, and returning None.
    """
    if url is None:
        return None
    url_hash = hashlib.md5(url.encode()).hexdigest()
    cache = f"cache/page_text_{url_hash}.txt"
    if os.path.exists(cache):
        with open(cache, "r", encoding="utf-8") as f:
            body = f.read()
        if body == "":
            return None
        return body
    try:
        logging.info("Crawling %s", url)
        response = requests.get(url, timeout=5)
        if response.status_code != 200:
            with open(cache, "w", encoding="utf-8") as f:
                f.write("")
            return None
        soup = BeautifulSoup(response.text, "html.parser")
        body = soup.get_text()

        with open(cache, "w", encoding="utf-8") as f:
            f.write(body)

        return body
    except Exception as e:
        logging.error("Error crawling %s: %s", url, e)
        with open(cache, "w", encoding="utf-8") as f:
            f.write("")
        return None


def get_stories(n=500):
    """
    Fetches the top stories from Hacker News, processes their content, and returns a list of story data.
    Args:
        n (int, optional): The number of top stories to fetch. Defaults to 500.
    Returns:
        list: A list of dictionaries, each containing the following keys:
            - 'id' (int): The unique ID of the story.
            - 'title' (str): The title of the story.
            - 'body' (str): The processed content of the story, including the title and a truncated version of the story's text.
            - 'url' (str): The URL of the story.
            - 'vec' (any): The vector representation of the story's content (generated by `get_embeddings`).
            - 'original_rank' (int): The original rank of the story in the top stories list.
    """
    url = "https://hacker-news.firebaseio.com/v0/topstories.json"
    response = requests.get(url, timeout=5)
    top_stories = response.json()[:n]

    logging.info("Getting stories.")
    stories = []

    def get_story(story_id):
        story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        story_response = requests.get(story_url, timeout=5)
        story_data = story_response.json()
        story_title = story_data.get("title")

        story_body = crawl_page(story_data.get("url"))
        if story_body is None:
            story_body = ""

        text = story_body
        # remove lines with < 200 characters
        text = " ".join([line for line in text.split("\n") if len(line) > 200])
        # take first 200k
        text = text[:200000]
        text = story_title + "\n" + text
        vec = get_embeddings(text)

        return {
            "id": story_data.get("id"),
            "title": story_title,
            "body": text,
            "url": story_data.get("url"),
            "vec": vec,
        }

    with ThreadPoolExecutor(10) as executor:
        stories = list(executor.map(get_story, top_stories))

    # add original ranks
    for i, story in enumerate(stories):
        story["original_rank"] = i + 1

    return stories


def re_rank(stories, profile):
    """
    Re-rank a list of stories based on their similarity to a given profile.

    This function calculates the cosine similarity between the profile's
    embedding vector and each story's embedding vector. It assigns a
    similarity score to each story, sorts the stories in descending order
    of their scores, and updates their rank and rank change.

    Args:
        stories (list of dict): A list of story dictionaries. Each dictionary
            must contain the keys:
            - 'vec': The embedding vector of the story.
            - 'original_rank': The original rank of the story.
        profile (str): A string representing the profile used for ranking.

    Returns:
        list of dict: The updated list of stories with the following keys:
            - 'score': The similarity score of the story.
            - 'rank': The new rank of the story after re-ranking.
            - 'change': The change in rank (original_rank - rank).
    """
    logging.info("Ranking stories.")
    profile_vec = get_embeddings(profile)
    for story in stories:
        story["score"] = util.pytorch_cos_sim(profile_vec, story["vec"]).item()
    stories.sort(key=lambda x: x["score"], reverse=True)
    for i, story in enumerate(stories):
        story["rank"] = i + 1
        story["change"] = story["original_rank"] - story["rank"]
    return stories


def get_embeddings(text):
    """
    Generate embeddings for a given text using a pre-trained model, with caching for efficiency.
    This function computes embeddings for the input text. If the embeddings for the text
    are already cached, it loads them from the cache to avoid recomputation. Otherwise, it
    computes the embeddings, saves them to the cache, and returns them. The embeddings are
    returned as a tensor, and if a GPU is available, the tensor is moved to the GPU.
    Args:
        text (str): The input text for which embeddings are to be generated.
    Returns:
        torch.Tensor: The embeddings for the input text as a tensor. The tensor is moved to
                      the GPU if available.
    """
    text_hash = hashlib.md5(text.encode()).hexdigest()
    # save to cache
    cache_file = f"cache/{text_hash}.npy"
    if os.path.exists(cache_file):
        # convert to tensor in gpu
        l = np.load(cache_file)
        l = torch.tensor(l)
        if torch.cuda.is_available():
            l = l.cuda()
        return l

    logging.info("Getting embeddings for %s", text)
    sentences = [text]
    embeddings = model.encode(sentences, convert_to_tensor=True)

    # save to cache
    np.save(cache_file, embeddings.cpu().numpy())

    return embeddings


@app.route("/")
def index():
    """
    Renders and returns the index page using the index template.

    Returns:
        str: The rendered HTML content of the index page.
    """
    return index_template.render()


@app.route("/news", methods=["POST"])
def get_news():
    """
    Fetches and processes news stories based on user input.
    This function retrieves a list of news stories, re-ranks them based on a
    user-provided profile, and returns the results in a rendered template.
    Returns:
        Response: A rendered HTML template containing the processed news stories
        or a JSON error message if the input is invalid.
    Input:
        - Expects a POST request with form data containing:
            - 'profile' (required): A string representing the user's profile for re-ranking.
            - 'number' (optional): An integer specifying the number of stories to fetch
              (default is 10, minimum is 1, maximum is 500).
    Output:
        - A list of dictionaries, each containing:
            - 'title': The title of the story.
            - 'url': The URL of the story.
            - 'original_rank': The original rank of the story.
            - 'rank': The re-ranked position of the story.
            - 'change': The change in rank after re-ranking.
            - 'score': The score of the story.
    Error Handling:
        - Returns a JSON response with an error message and a 400 status code
          if the 'profile' key is missing from the input.
    """
    form_data = request.form
    if "profile" not in form_data:
        return jsonify({"error": "Invalid input"}), 400
    num = 10
    if "number" in form_data:
        num = int(form_data["number"])
    if num > 500:
        num = 500
    if num < 1:
        num = 1

    stories = get_stories(num)

    stories = re_rank(stories, form_data["profile"])

    # take only title, url, original_rank and rank
    stories = [
        {
            "title": story["title"],
            "url": story["url"],
            "original_rank": story["original_rank"],
            "rank": story["rank"],
            "change": story["change"],
            "score": story["score"],
        }
        for story in stories
    ]

    return results_template.render(results=stories)


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)
